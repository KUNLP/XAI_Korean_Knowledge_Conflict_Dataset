from itertools import islice
import json
import os
import re
import torch
from tqdm import tqdm
from typing import Optional, List, Dict, Any
from utils import load_data

class ModelEvaluator:
    # ëª¨ë¸ í‰ê°€ë¥¼ ìœ„í•œ í´ë˜ìŠ¤
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self.labels = ["A", "B", "C", "D"]
        self.correct_count = 0
        self.wrong_count = 0
        self.VR = 0
        self.RR = 0
        self.DMSS = 0

    # ëª¨ë¸ ì¶œë ¥ì—ì„œ ì •ë‹µ ì¶”ì¶œ
    def clean_prediction(self, raw_output: str) -> str:
        raw_output = raw_output.strip().upper()
        for pattern in [r"\b([A-D])\b", r"\b([A-D])[).:,\s]", r"\b([A-D])"]:
            match = re.search(pattern, raw_output)
            if match:
                return match.group(1)
        return "INVALID"

    # 1. ëª¨ë¸ í‰ê°€ í”„ë¡¬í”„íŠ¸ ìƒì„± (ì§ˆë¬¸ë§Œ)
    def create_prompt_question(self, question: str, choices: List[str]) -> str:
        formatted_choices = "\n".join([f"{self.labels[i]}. {choice}" for i, choice in enumerate(choices)])
        
        prompt = f"""[ì§€ì‹œ]
ì§ˆë¬¸ì„ ì½ê³  ì„ íƒì§€ ì¤‘ì—ì„œ ê°€ì¥ ì ì ˆí•œ ì •ë‹µì„ í•˜ë‚˜ë§Œ ì„ íƒí•˜ì„¸ìš”.  
ì •ë‹µì€ ë°˜ë“œì‹œ A, B, C, D ì¤‘ ì•ŒíŒŒë²³ í•œ ê¸€ìë§Œ ì¶œë ¥í•˜ì„¸ìš”.  
ì ˆëŒ€ í•´ì„¤ì´ë‚˜ ì„¤ëª…ì€ ì“°ì§€ ë§ˆì„¸ìš”.  
ì •ë‹µë§Œ ì¶œë ¥í•˜ì„¸ìš”.

[ì§ˆë¬¸]: 
{question}

[ì„ íƒì§€]
{formatted_choices}
### ì •ë‹µ:"""
        return prompt
    # 2. ëª¨ë¸ í‰ê°€ í”„ë¡¬í”„íŠ¸ ìƒì„± (ì§ˆë¬¸ + ë¬¸ë§¥)
    def create_prompt_question_with_context(self, question: str, choices: List[str], context: str) -> str:
        formatted_choices = "\n".join([f"{self.labels[i]}. {choice}" for i, choice in enumerate(choices)])
            
        prompt = f"""[ì§€ì‹œ]
ì§ˆë¬¸ì„ ì½ê³  ë¬¸ë§¥ì„ ì°¸ê³ í•˜ì—¬ ì„ íƒì§€ ì¤‘ì—ì„œ ê°€ì¥ ì ì ˆí•œ ì •ë‹µì„ í•˜ë‚˜ë§Œ ì„ íƒí•˜ì„¸ìš”.  
ì •ë‹µì€ ë°˜ë“œì‹œ A, B, C, D ì¤‘ ì•ŒíŒŒë²³ í•œ ê¸€ìë§Œ ì¶œë ¥í•˜ì„¸ìš”.  
ì ˆëŒ€ í•´ì„¤ì´ë‚˜ ì„¤ëª…ì€ ì“°ì§€ ë§ˆì„¸ìš”.  
ì •ë‹µë§Œ ì¶œë ¥í•˜ì„¸ìš”.

[ë¬¸ë§¥]:
{context}

[ì§ˆë¬¸]: 
{question}

[ì„ íƒì§€]
{formatted_choices}
### ì •ë‹µ:"""
        return prompt

    # ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰    
    def run_model(self, question: str, choices: List[str], context: Optional[str] = None) -> str:
        if context is None:
            prompt = self.create_prompt_question(question, choices)
        else:
            prompt = self.create_prompt_question_with_context(question, choices, context)
        
        # ëª¨ë¸ ì…ë ¥ êµ¬ì„±
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
        input_len = inputs["input_ids"].shape[-1]
        
        # EOS í† í° ì²´í¬
        # if self.tokenizer.eos_token_id is not None:
        #     print("EOS í† í° ì¡´ì¬")
        # else:
        #     print("EOS í† í° ì—†ìŒ")

        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=5,
                do_sample=False,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        # ìƒì„±ëœ í† í° ì¤‘ prompt ì´í›„ë§Œ ë””ì½”ë”©
        generated_only = outputs[0][input_len:]
        decoded = self.tokenizer.decode(generated_only, skip_special_tokens=True)
        
        print("ğŸ§© ëª¨ë¸ ì‘ë‹µ:", decoded)
        predicted = self.clean_prediction(decoded)
        print("ğŸ§¼ ìµœì¢… predicted:", predicted)
        print(f"prompt : \n{prompt}")
        print("--------------------------------"
        )
        return predicted
    
    def evaluate_question(self, item: Dict[str, Any]) -> Dict[str, Any]:
        question = item["question"]
        choices = item["choices"]
        answer = item["answer"]
        predicted = self.run_model(question, choices)
        memory_result = "correct" if predicted == answer else "wrong"
        
        result = {
            "question": item["question"],
            "answer": item["answer"],
            "negative_answer": item["negative_answer"],
            "candidate": item["candidate"],
            "golden_context": item["golden_context"],            
            "negative_context": item["negative_context"],
            "choices": item["choices"],
            "memory_predicted": predicted,
            "memory_result": memory_result
        }
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        if memory_result == "correct":
            self.correct_count += 1
        else:
            self.wrong_count += 1
            
        return result
    
    def evaluate_question_with_context(self, question_result: Dict[str, Any]) -> Dict[str, Any]:
        question = question_result["question"]
        choices = question_result["choices"]
        answer = question_result["answer"]
        if question_result["memory_result"] == "correct":
            context = question_result["negative_context"]
        elif question_result["memory_result"] == "wrong":
            context = question_result["golden_context"]
        predicted = self.run_model(question, choices, context)
        return predicted

    
    def evaluate_metrics(self, data):
        D_plus = [item for item in data if item["memory_result"] == "correct"]
        D_minus = [item for item in data if item["memory_result"] == "wrong"]
        self.VR = sum(item["negative_context_predicted"] == item["answer"] for item in D_plus) / len(D_plus) if D_plus else 0.0
        self.RR = sum(item["golden_context_predicted"] == item["answer"] for item in D_minus) / len(D_minus) if D_minus else 0.0
        # DMSSì˜ 1ì°¨ í•­
        term1 = sum(item["negative_context_predicted"] == item["answer"] for item in D_plus) + sum(item["golden_context_predicted"] == item["memory_predicted"] for item in D_minus)
        # DMSSì˜ 2ì°¨ í•­
        term2 = sum(item["negative_context_predicted"] == item["candidate"] for item in D_plus) + sum(item["golden_context_predicted"] == item["answer"] for item in D_minus)
        self.DMSS = (term1 - term2) / len(data) if data else 0.0
        print(f"ğŸ“Š VR ê°œìˆ˜: {len(D_plus)}")
        print(f"ğŸ“Š VR: {self.VR:.2f}%")
        print(f"ğŸ“Š RR ê°œìˆ˜: {len(D_minus)}")
        print(f"ğŸ“Š RR: {self.RR:.2f}%")
        print(f"ğŸ“Š DMSS ê°œìˆ˜: {term1} + {term2} = {term1 + term2}")
        print(f"ğŸ“Š DMSS: {self.DMSS:.2f}%")
        return {
            "PA": len(D_plus) / (len(D_plus) + len(D_minus)),
            "VR": self.VR, 
            "RR": self.RR, 
            "DMSS": self.DMSS,
            "counts" :{
                "data_len" : len(D_plus) + len(D_minus),
                "data_question_correct(VR_len)" : len(D_plus),
                "data_question_wrong(RR_len)" : len(D_minus),
                "DMSS_term1" : term1,
                "DMSS_term2" : term2,
                "DMSS_len" : term1 + term2
            }}

    def evaluate_dataset(self, data: List[Dict[str, Any]], model_name: str, max_samples: int = None):
        """ì „ì²´ ë°ì´í„°ì…‹ í‰ê°€"""
        # ì´ ë¼ì¸ ìˆ˜ ê³„ì‚°
        total_lines = len(data)
        
        if max_samples:
            total_lines = min(total_lines, max_samples)

        # ê²°ê³¼ ì €ì¥ (í˜„ì¬ ë””ë ‰í† ë¦¬ì— ì €ì¥)
        output_dir = f"results/{model_name}"
        os.makedirs(output_dir, exist_ok=True)
        output_file = f"{output_dir}/{model_name}_evaluation_results.jsonl"

        # í‰ê°€ ì‹¤í–‰
        for item in tqdm(data, total=total_lines, desc=f"Evaluating ({total_lines} samples)"):
            question_result = self.evaluate_question(item)
            if question_result["memory_result"] == "correct":
                negative_context_predicted = self.evaluate_question_with_context(question_result)
            elif question_result["memory_result"] == "wrong":
                golden_context_predicted = self.evaluate_question_with_context(question_result)
            question_result.update({
                "negative_context_predicted": negative_context_predicted if question_result["memory_result"] == "correct" else None,
                "golden_context_predicted": golden_context_predicted if question_result["memory_result"] == "wrong" else None
            })            
            with open(output_file, "a", encoding="utf-8") as f:
                f.write(json.dumps(question_result, ensure_ascii=False) + "\n")
        
        print(f"\nâœ… ë§ì¶˜ ê°œìˆ˜: {self.correct_count} / í‹€ë¦° ê°œìˆ˜: {self.wrong_count}")
        print(f"ğŸ“Š ì •í™•ë„: {self.correct_count / (self.correct_count + self.wrong_count) * 100:.2f}%")

        metrics_data = load_data(output_file)
        metrics = self.evaluate_metrics(metrics_data)
        metrics_path = f"{output_dir}/{model_name}_metrics.jsonl"
        with open(metrics_path, "w", encoding="utf-8") as mf:
            json.dump(metrics, mf, ensure_ascii=False, indent=2)
        print(f"ğŸ“ Saved metrics to {metrics_path}")