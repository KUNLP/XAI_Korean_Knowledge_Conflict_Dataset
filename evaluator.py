from itertools import islice
import json
import os
import re
import torch
from tqdm import tqdm
from typing import List, Dict, Any

class ModelEvaluator:
    # ëª¨ë¸ í‰ê°€ë¥¼ ìœ„í•œ í´ë˜ìŠ¤
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self.labels = ["A", "B", "C", "D"]
        self.correct_count = 0
        self.wrong_count = 0
    
    # ëª¨ë¸ ì¶œë ¥ì—ì„œ ì •ë‹µ ì¶”ì¶œ
    def clean_prediction(self, raw_output: str) -> str:
        raw_output = raw_output.strip().upper()
        for pattern in [r"\b([A-D])\b", r"\b([A-D])[).:,\s]", r"\b([A-D])"]:
            match = re.search(pattern, raw_output)
            if match:
                return match.group(1)
        return "INVALID"
    # í”„ë¡¬í”„íŠ¸ ìƒì„±
    def create_prompt(self, question: str, choices: List[str]) -> str:
        formatted_choices = "\n".join([f"{self.labels[i]}. {choice}" for i, choice in enumerate(choices)])
        
        prompt = f"""[ì§€ì‹œ]
ì„ íƒì§€ ì¤‘ì—ì„œ ê°€ì¥ ì ì ˆí•œ ì •ë‹µì„ í•˜ë‚˜ë§Œ ì„ íƒí•˜ì„¸ìš”.  
ì •ë‹µì€ ë°˜ë“œì‹œ A, B, C, D ì¤‘ ì•ŒíŒŒë²³ í•œ ê¸€ìë§Œ ì¶œë ¥í•˜ì„¸ìš”.  
ì ˆëŒ€ í•´ì„¤ì´ë‚˜ ì„¤ëª…ì€ ì“°ì§€ ë§ˆì„¸ìš”.  
ì •ë‹µë§Œ ì¶œë ¥í•˜ì„¸ìš”.

[ì§ˆë¬¸]: 
{question}

[ì„ íƒì§€]
{formatted_choices}
### ì •ë‹µ:"""
        return prompt
    # ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰    
    def run_model(self, question: str, choices: List[str]) -> str:
        prompt = self.create_prompt(question, choices)
        
        # ëª¨ë¸ ì…ë ¥ êµ¬ì„±
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
        input_len = inputs["input_ids"].shape[-1]
        
        # EOS í† í° ì²´í¬
        if self.tokenizer.eos_token_id is not None:
            print("EOS í† í° ì¡´ì¬")
        else:
            print("EOS í† í° ì—†ìŒ")

        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=5,
                do_sample=False,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        # ìƒì„±ëœ í† í° ì¤‘ prompt ì´í›„ë§Œ ë””ì½”ë”©
        generated_only = outputs[0][input_len:]
        decoded = self.tokenizer.decode(generated_only, skip_special_tokens=True)
        
        print("ğŸ§© ëª¨ë¸ ì‘ë‹µ:", decoded)
        predicted = self.clean_prediction(decoded)
        print("ğŸ§¼ ìµœì¢… predicted:", predicted)
        
        return predicted
    
    def evaluate_single_item(self, item: Dict[str, Any]) -> Dict[str, Any]:
        """ë‹¨ì¼ ì•„ì´í…œ í‰ê°€"""
        question = item["question"]
        choices = item["choices"]
        answer = item["answer"]
        candidate = item["candidate"]
        predicted = self.run_model(question, choices)
        print("í•„í„°ë§ ëª¨ë¸ ì‘ë‹µ:", predicted)
        
        memory_result = "correct" if predicted == answer else "wrong"
        
        result = {
            "question": item["question"],
            "answer": item["answer"],
            "negative_answer": item["negative_answer"],
            "candidate": item["candidate"],
            "golden_context": item["golden_context"],            
            "negative_context": item["negative_context"],
            "choices": item["choices"],
            "memory_predicted": predicted,
            "memory_result": memory_result
        }
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        if memory_result == "correct":
            self.correct_count += 1
        else:
            self.wrong_count += 1
            
        return result
    
    def evaluate_dataset(self, input_file: str, model_name: str, max_samples: int = None):
        """ì „ì²´ ë°ì´í„°ì…‹ í‰ê°€"""
        # ì´ ë¼ì¸ ìˆ˜ ê³„ì‚°
        with open(input_file, "r", encoding="utf-8") as f:
            total_lines = sum(1 for _ in f)
        
        if max_samples:
            total_lines = min(total_lines, max_samples)
        
        # í‰ê°€ ì‹¤í–‰
        with open(input_file, "r", encoding="utf-8") as f:
            for line in tqdm(f, total=total_lines, desc=f"Evaluating ({total_lines} samples)"):
            # for line in tqdm(islice(f, 20), total = 20, desc=f"Evaluating (20 samples)"):
                item = json.loads(line)
                result = self.evaluate_single_item(item)
                
                # ê²°ê³¼ ì €ì¥ (í˜„ì¬ ë””ë ‰í† ë¦¬ì— ì €ì¥)
                output_dir = f"results/{model_name}"
                os.makedirs(output_dir, exist_ok=True)
                output_file = f"{output_dir}/{model_name}_evaluation_results.jsonl"
                
                with open(output_file, "a", encoding="utf-8") as f:
                    f.write(json.dumps(result, ensure_ascii=False) + "\n")
        
        print(f"\nâœ… ë§ì¶˜ ê°œìˆ˜: {self.correct_count} / í‹€ë¦° ê°œìˆ˜: {self.wrong_count}")
        print(f"ğŸ“Š ì •í™•ë„: {self.correct_count / (self.correct_count + self.wrong_count) * 100:.2f}%")
